{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "# правильно выбирайте карту, иначе все упадет!\n",
    "CUDA_DEVICE=0\n",
    "\n",
    "def data2image(data):\n",
    "    res = np.transpose(np.reshape(data ,(3, 32,32)), (1,2,0))\n",
    "    return PIL.Image.fromarray(np.uint8(res))\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy().astype('uint8')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomC100Dataset(data.Dataset):\n",
    "    def __init__(self, dataset_path, dataset_type, img_cnt = 50000, transform = None, target_transform = None):\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise \"Unknown dataset type : {}\".format(dataset_type)\n",
    "        self.ds_type = dataset_type\n",
    "        self.ds_path = dataset_path\n",
    "        self.img_cnt = img_cnt\n",
    "        self.transform = transform\n",
    "        self.t_transform = target_transform\n",
    "        self.__load__()\n",
    "    \n",
    "    def __load__(self):\n",
    "        dataset = np.load(self.ds_path)\n",
    "        if self.ds_type == 'train':\n",
    "            dataset = dataset.reshape((self.img_cnt,3073))\n",
    "            self.y, self.x = np.hsplit(dataset, [1])\n",
    "            self.y = self.y.astype(np.int64)\n",
    "            self.x = self.x.reshape((self.x.shape[0],3,32,32))\n",
    "            self.x = self.x.transpose((0, 2, 3, 1))\n",
    "        \n",
    "        if self.ds_type == 'test':\n",
    "            dataset = dataset.reshape((self.img_cnt,3072))\n",
    "            self.y = np.zeros((dataset.shape[0], 1), dtype=np.int64)\n",
    "            self.x = dataset.reshape((dataset.shape[0],3,32,32))\n",
    "            self.x = self.x.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.x[index], self.y[index]\n",
    "        \n",
    "        img = PIL.Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.t_transform is not None:\n",
    "            target = self.t_transform(target)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # тут надо сделать аугментации и преобразования для trainset\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    # тут так же преобразования, но для testset\n",
    "])\n",
    "\n",
    "# подумайте, должны ли преобразования train и test сет быть абсолютно одинаковыми ?\n",
    "\n",
    "train_dataset = CustomC100Dataset('homework_4.train.npy', 'train', img_cnt = 50000, transform = transform_train)\n",
    "test_nc_dataset = CustomC100Dataset('homework_4_no_classes.test.npy', 'test', img_cnt = 10000, transform = transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[4]\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        # конструктор базового блока\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # форвард базового блока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы вы не запутались, сборку самой модели предоставляем\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = # вставить сюда стартовую конволюцию из статьи\n",
    "        self.bn1 = # bn-слой\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # написать инициализацию параметров конволюции\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # инициализация параметров батч-нормализаций\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                # здесь необходимо привести слои для downsampling в shotrcut соединениях\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбирайте любой ResNet\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3,3,3], 100)\n",
    "\n",
    "def ResNet32():\n",
    "    return ResNet(BasicBlock, [5,5,5], 100)\n",
    "\n",
    "def ResNet44():\n",
    "    return ResNet(BasicBlock, [7,7,7], 100)\n",
    "\n",
    "def ResNet56():\n",
    "    return ResNet(BasicBlock, [9,9,9], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_check(model, validation_loader, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(validation_loader):\n",
    "        inputs, labels = inputs.cuda(), labels.view(batch_size).cuda()\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs)\n",
    "        prob, predicted = torch.topk(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.view(-1).eq(labels.data).sum()\n",
    "        \n",
    "    model.train()\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_solution_pytorch(model, test_data_loader, batch_size):\n",
    "    res = []\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_data_loader):\n",
    "        inputs, labels = inputs.cuda(), labels.view(batch_size).cuda()\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs)\n",
    "        prob, predicted = torch.topk(outputs.data, 1)\n",
    "        res = np.append(res, predicted.view(-1).cpu().numpy())\n",
    "    model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ResNetXX()\n",
    "model.cuda(CUDA_DEVICE)\n",
    "\n",
    "#all parameters from https://arxiv.org/pdf/1512.03385.pdf\n",
    "epochs_cnt = # кол-во эпох\n",
    "\n",
    "# эти параметры тоже на ваше усмотрение\n",
    "minibatch_size = 125 \n",
    "test_batch_size = 100\n",
    "\n",
    "optimizer = # выбираем отимизатор \n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda(CUDA_DEVICE)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = minibatch_size, shuffle = True, num_workers = 2)\n",
    "test_nc_loader = torch.utils.data.DataLoader(test_nc_dataset, batch_size = test_batch_size, shuffle = False, num_workers = 2)\n",
    "#losses = []\n",
    "\n",
    "# примерный код для обучения модели, не обязательно строго следовать ему\n",
    "for epoch in range(epochs_cnt):    \n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        if(batch_idx % 10 == 0):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, train_loss / 10))\n",
    "            train_loss = 0\n",
    "        inputs, labels = inputs.cuda(), labels.view(minibatch_size).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерим решение вашей модели\n",
    "solution = make_solution_pytorch(model, test_nc_loader, test_batch_size)\n",
    "with open('my_solution.csv', 'w') as fout:\n",
    "    print('Id', 'Prediction', sep=',', file=fout)\n",
    "    for i, prediction in enumerate(solution):\n",
    "        print(i, int(prediction), sep=',', file=fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
