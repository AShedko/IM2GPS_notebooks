{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrainedmodels.utils as utils\n",
    "import pretrainedmodels\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.linspace(0,10,100),device=torch.device(\"cuda\"),\n",
    "                requires_grad = True)\n",
    "t = t.reshape((10,10))\n",
    "t2 = t * t\n",
    "t2 =  torch.matmul(t2, torch.ones((10,1), dtype=torch.double))\n",
    "t3 = t2.t() @ (5* torch.ones(10, dtype=torch.double))\n",
    "t3.backward()\n",
    "g = t3.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def double_backprop(inputs, net):\n",
    "    y = net(x).mean()\n",
    "    grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)\n",
    "    return grad.pow(2).mean() + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "x = torch.ones(1) \n",
    "tot = w + x\n",
    "tot.backward()\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fbresnet152' # could be fbresnet152 or inceptionresnetv2\n",
    "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "model.eval()\n",
    "\n",
    "load_img = utils.LoadImage()\n",
    "\n",
    "# transformations depending on the model\n",
    "# rescale, center crop, normalize, and others (ex: ToBGR, ToRange255)\n",
    "tf_img = utils.TransformImage(model) \n",
    "\n",
    "path_img = 'cat.jpeg'\n",
    "\n",
    "input_img = load_img(path_img)\n",
    "input_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ\n",
    "input_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299\n",
    "input = torch.autograd.Variable(input_tensor,\n",
    "    requires_grad=False)\n",
    "\n",
    "output_logits = model(input) # 1x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8.3944,  7.4880,  7.3134,  6.4543,  6.1048]]),\n",
       " tensor([[ 285,  281,  284,  283,  282]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits.topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__builtins__': {'ArithmeticError': ArithmeticError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'BaseException': BaseException,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'BufferError': BufferError,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'EOFError': EOFError,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'EnvironmentError': OSError,\n",
       "  'Exception': Exception,\n",
       "  'False': False,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'IOError': OSError,\n",
       "  'ImportError': ImportError,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'IndentationError': IndentationError,\n",
       "  'IndexError': IndexError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'KeyError': KeyError,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'LookupError': LookupError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'NameError': NameError,\n",
       "  'None': None,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'OSError': OSError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'SystemError': SystemError,\n",
       "  'SystemExit': SystemExit,\n",
       "  'TabError': TabError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'True': True,\n",
       "  'TypeError': TypeError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'ValueError': ValueError,\n",
       "  'Warning': Warning,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  '__IPYTHON__': True,\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__debug__': True,\n",
       "  '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\",\n",
       "  '__import__': <function __import__>,\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__name__': 'builtins',\n",
       "  '__package__': '',\n",
       "  '__pybind11_internals_v1__': <capsule object NULL at 0x7f23a8064a50>,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>),\n",
       "  'abs': <function abs>,\n",
       "  'all': <function all>,\n",
       "  'any': <function any>,\n",
       "  'ascii': <function ascii>,\n",
       "  'bin': <function bin>,\n",
       "  'bool': bool,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'callable': <function callable>,\n",
       "  'chr': <function chr>,\n",
       "  'classmethod': classmethod,\n",
       "  'compile': <function compile>,\n",
       "  'complex': complex,\n",
       "  'copyright': Copyright (c) 2001-2017 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "      for supporting Python development.  See www.python.org for more information.,\n",
       "  'delattr': <function delattr>,\n",
       "  'dict': dict,\n",
       "  'dir': <function dir>,\n",
       "  'display': <function IPython.core.display.display>,\n",
       "  'divmod': <function divmod>,\n",
       "  'enumerate': enumerate,\n",
       "  'eval': <function eval>,\n",
       "  'exec': <function exec>,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'format': <function format>,\n",
       "  'frozenset': frozenset,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f23ab7ffe48>>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals>,\n",
       "  'hasattr': <function hasattr>,\n",
       "  'hash': <function hash>,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  'hex': <function hex>,\n",
       "  'id': <function id>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f23ab7ffdd8>>,\n",
       "  'int': int,\n",
       "  'isinstance': <function isinstance>,\n",
       "  'issubclass': <function issubclass>,\n",
       "  'iter': <function iter>,\n",
       "  'len': <function len>,\n",
       "  'license': Type license() to see the full license text,\n",
       "  'list': list,\n",
       "  'locals': <function locals>,\n",
       "  'map': map,\n",
       "  'max': <function max>,\n",
       "  'memoryview': memoryview,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'object': object,\n",
       "  'oct': <function oct>,\n",
       "  'open': <function io.open>,\n",
       "  'ord': <function ord>,\n",
       "  'pow': <function pow>,\n",
       "  'print': <function print>,\n",
       "  'property': property,\n",
       "  'range': range,\n",
       "  'repr': <function repr>,\n",
       "  'reversed': reversed,\n",
       "  'round': <function round>,\n",
       "  'set': set,\n",
       "  'setattr': <function setattr>,\n",
       "  'slice': slice,\n",
       "  'sorted': <function sorted>,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'sum': <function sum>,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'vars': <function vars>,\n",
       "  'zip': zip},\n",
       " '__cached__': '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/__pycache__/__init__.cpython-36.pyc',\n",
       " '__doc__': None,\n",
       " '__file__': '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/__init__.py',\n",
       " '__loader__': <_frozen_importlib_external.SourceFileLoader at 0x7f233fa0d320>,\n",
       " '__name__': 'pretrainedmodels',\n",
       " '__package__': 'pretrainedmodels',\n",
       " '__path__': ['/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels'],\n",
       " '__spec__': ModuleSpec(name='pretrainedmodels', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f233fa0d320>, origin='/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/__init__.py', submodule_search_locations=['/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels']),\n",
       " '__version__': '0.6.0',\n",
       " 'alexnet': <function pretrainedmodels.models.torchvision_models.alexnet>,\n",
       " 'bninception': <function pretrainedmodels.models.bninception.bninception>,\n",
       " 'cafferesnet101': <function pretrainedmodels.models.cafferesnet.cafferesnet101>,\n",
       " 'datasets': <module 'pretrainedmodels.datasets' from '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/datasets/__init__.py'>,\n",
       " 'densenet121': <function pretrainedmodels.models.torchvision_models.densenet121>,\n",
       " 'densenet161': <function pretrainedmodels.models.torchvision_models.densenet161>,\n",
       " 'densenet169': <function pretrainedmodels.models.torchvision_models.densenet169>,\n",
       " 'densenet201': <function pretrainedmodels.models.torchvision_models.densenet201>,\n",
       " 'dpn107': <function pretrainedmodels.models.dpn.dpn107>,\n",
       " 'dpn131': <function pretrainedmodels.models.dpn.dpn131>,\n",
       " 'dpn68': <function pretrainedmodels.models.dpn.dpn68>,\n",
       " 'dpn68b': <function pretrainedmodels.models.dpn.dpn68b>,\n",
       " 'dpn92': <function pretrainedmodels.models.dpn.dpn92>,\n",
       " 'dpn98': <function pretrainedmodels.models.dpn.dpn98>,\n",
       " 'fbresnet152': <function pretrainedmodels.models.fbresnet.fbresnet152>,\n",
       " 'inceptionresnetv2': <function pretrainedmodels.models.inceptionresnetv2.inceptionresnetv2>,\n",
       " 'inceptionv3': <function pretrainedmodels.models.torchvision_models.inceptionv3>,\n",
       " 'inceptionv4': <function pretrainedmodels.models.inceptionv4.inceptionv4>,\n",
       " 'model_names': ['fbresnet152',\n",
       "  'bninception',\n",
       "  'resnext101_32x4d',\n",
       "  'resnext101_64x4d',\n",
       "  'inceptionv4',\n",
       "  'inceptionresnetv2',\n",
       "  'alexnet',\n",
       "  'densenet121',\n",
       "  'densenet169',\n",
       "  'densenet201',\n",
       "  'densenet161',\n",
       "  'resnet18',\n",
       "  'resnet34',\n",
       "  'resnet50',\n",
       "  'resnet101',\n",
       "  'resnet152',\n",
       "  'inceptionv3',\n",
       "  'squeezenet1_0',\n",
       "  'squeezenet1_1',\n",
       "  'vgg11',\n",
       "  'vgg11_bn',\n",
       "  'vgg13',\n",
       "  'vgg13_bn',\n",
       "  'vgg16',\n",
       "  'vgg16_bn',\n",
       "  'vgg19_bn',\n",
       "  'vgg19',\n",
       "  'nasnetamobile',\n",
       "  'nasnetalarge',\n",
       "  'dpn68',\n",
       "  'dpn68b',\n",
       "  'dpn92',\n",
       "  'dpn98',\n",
       "  'dpn131',\n",
       "  'dpn107',\n",
       "  'xception',\n",
       "  'senet154',\n",
       "  'se_resnet50',\n",
       "  'se_resnet101',\n",
       "  'se_resnet152',\n",
       "  'se_resnext50_32x4d',\n",
       "  'se_resnext101_32x4d'],\n",
       " 'models': <module 'pretrainedmodels.models' from '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/models/__init__.py'>,\n",
       " 'nasnetalarge': <function pretrainedmodels.models.nasnet.nasnetalarge>,\n",
       " 'nasnetamobile': <function pretrainedmodels.models.nasnet_mobile.nasnetamobile>,\n",
       " 'pretrained_settings': {'alexnet': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth'}},\n",
       "  'bninception': {'imagenet': {'input_range': [0, 255],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'BGR',\n",
       "    'mean': [104, 117, 128],\n",
       "    'num_classes': 1000,\n",
       "    'std': [1, 1, 1],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/bn_inception-239d2248.pth'}},\n",
       "  'densenet121': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/densenet121-241335ed.pth'}},\n",
       "  'densenet161': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/densenet161-17b70270.pth'}},\n",
       "  'densenet169': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/densenet169-6f0f7f60.pth'}},\n",
       "  'densenet201': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/densenet201-4c113574.pth'}},\n",
       "  'dpn107': {'imagenet+5k': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn107_extra-1ac7121e2.pth'}},\n",
       "  'dpn131': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn131-71dfe43e0.pth'}},\n",
       "  'dpn68': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn68-66bebafa7.pth'}},\n",
       "  'dpn68b': {'imagenet+5k': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn68b_extra-84854c156.pth'}},\n",
       "  'dpn92': {'imagenet+5k': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn92_extra-b040e4a9b.pth'}},\n",
       "  'dpn98': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.48627450980392156, 0.4588235294117647, 0.40784313725490196],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.23482446870963955, 0.23482446870963955, 0.23482446870963955],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/dpn98-5b90dec4d.pth'}},\n",
       "  'fbresnet152': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/fbresnet152-2e20f6b4.pth'}},\n",
       "  'inceptionresnetv2': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth'},\n",
       "   'imagenet+background': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1001,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth'}},\n",
       "  'inceptionv3': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'}},\n",
       "  'inceptionv4': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth'},\n",
       "   'imagenet+background': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1001,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth'}},\n",
       "  'nasnetalarge': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 331, 331],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'},\n",
       "   'imagenet+background': {'input_range': [0, 1],\n",
       "    'input_size': [3, 331, 331],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1001,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'}},\n",
       "  'nasnetamobile': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetamobile-7e03cead.pth'}},\n",
       "  'resnet101': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'}},\n",
       "  'resnet152': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth'}},\n",
       "  'resnet18': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'}},\n",
       "  'resnet34': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'}},\n",
       "  'resnet50': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/resnet50-19c8e357.pth'}},\n",
       "  'resnext101_32x4d': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_32x4d-29e315fa.pth'}},\n",
       "  'resnext101_64x4d': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth'}},\n",
       "  'se_resnet101': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth'}},\n",
       "  'se_resnet152': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth'}},\n",
       "  'se_resnet50': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth'}},\n",
       "  'se_resnext101_32x4d': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth'}},\n",
       "  'se_resnext50_32x4d': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth'}},\n",
       "  'senet154': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth'}},\n",
       "  'squeezenet1_0': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth'}},\n",
       "  'squeezenet1_1': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth'}},\n",
       "  'vgg11': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth'}},\n",
       "  'vgg11_bn': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth'}},\n",
       "  'vgg13': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg13-c768596a.pth'}},\n",
       "  'vgg13_bn': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth'}},\n",
       "  'vgg16': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg16-397923af.pth'}},\n",
       "  'vgg16_bn': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth'}},\n",
       "  'vgg19': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth'}},\n",
       "  'vgg19_bn': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 224, 224],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.485, 0.456, 0.406],\n",
       "    'num_classes': 1000,\n",
       "    'std': [0.229, 0.224, 0.225],\n",
       "    'url': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth'}},\n",
       "  'xception': {'imagenet': {'input_range': [0, 1],\n",
       "    'input_size': [3, 299, 299],\n",
       "    'input_space': 'RGB',\n",
       "    'mean': [0.5, 0.5, 0.5],\n",
       "    'num_classes': 1000,\n",
       "    'scale': 0.8975,\n",
       "    'std': [0.5, 0.5, 0.5],\n",
       "    'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth'}}},\n",
       " 'resnet101': <function pretrainedmodels.models.torchvision_models.resnet101>,\n",
       " 'resnet152': <function pretrainedmodels.models.torchvision_models.resnet152>,\n",
       " 'resnet18': <function pretrainedmodels.models.torchvision_models.resnet18>,\n",
       " 'resnet34': <function pretrainedmodels.models.torchvision_models.resnet34>,\n",
       " 'resnet50': <function pretrainedmodels.models.torchvision_models.resnet50>,\n",
       " 'resnext101_32x4d': <function pretrainedmodels.models.resnext.resnext101_32x4d>,\n",
       " 'resnext101_64x4d': <function pretrainedmodels.models.resnext.resnext101_64x4d>,\n",
       " 'se_resnet101': <function pretrainedmodels.models.senet.se_resnet101>,\n",
       " 'se_resnet152': <function pretrainedmodels.models.senet.se_resnet152>,\n",
       " 'se_resnet50': <function pretrainedmodels.models.senet.se_resnet50>,\n",
       " 'se_resnext101_32x4d': <function pretrainedmodels.models.senet.se_resnext101_32x4d>,\n",
       " 'se_resnext50_32x4d': <function pretrainedmodels.models.senet.se_resnext50_32x4d>,\n",
       " 'senet154': <function pretrainedmodels.models.senet.senet154>,\n",
       " 'squeezenet1_0': <function pretrainedmodels.models.torchvision_models.squeezenet1_0>,\n",
       " 'squeezenet1_1': <function pretrainedmodels.models.torchvision_models.squeezenet1_1>,\n",
       " 'utils': <module 'pretrainedmodels.utils' from '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/utils.py'>,\n",
       " 'version': <module 'pretrainedmodels.version' from '/home/ashedko/anaconda3/lib/python3.6/site-packages/pretrainedmodels/version.py'>,\n",
       " 'vgg11': <function pretrainedmodels.models.torchvision_models.vgg11>,\n",
       " 'vgg11_bn': <function pretrainedmodels.models.torchvision_models.vgg11_bn>,\n",
       " 'vgg13': <function pretrainedmodels.models.torchvision_models.vgg13>,\n",
       " 'vgg13_bn': <function pretrainedmodels.models.torchvision_models.vgg13_bn>,\n",
       " 'vgg16': <function pretrainedmodels.models.torchvision_models.vgg16>,\n",
       " 'vgg16_bn': <function pretrainedmodels.models.torchvision_models.vgg16_bn>,\n",
       " 'vgg19': <function pretrainedmodels.models.torchvision_models.vgg19>,\n",
       " 'vgg19_bn': <function pretrainedmodels.models.torchvision_models.vgg19_bn>,\n",
       " 'xception': <function pretrainedmodels.models.xception.xception>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrainedmodels.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
